
environment:
  height: 6
  width: 6
  init state: [0,0,0]
  # action uncertainty is over estimated for learning
  real action uncertainty: 0.10
  over estimated action uncertainty: 0.11


TWTL constraint:
  pickup: [4,4,0]
  delivery: [0,1,0]
  desired satisfaction probability: 0.85
  custom task: None
  time horizon: 30

# MDP type: flag-MDP
# MDP type: tau-MDP
MDP type: static rewards

# Used for flag-MDP and tau-MDP augmentations
aug-MDP rewards:
  STL expression: G[0,10]F[0,3](((x>1)&(x<2))&((y>3)&(y<4)))

static rewards:
  reward dict: 
    r9: 5
    r32: 5

Q-learning config:
  number of episodes: 50000
  learning rate: 0.1
  discount: 0.999
  # Exploration probability decay is calculated from the following values as (start * decay^(num_eps - 1) = end)
  explore probability start: 0.4
  explore probability end: 0.07
