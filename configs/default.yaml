
environment:
  height: 8
  width: 8
  init state: [0,0,0]
  # action uncertainty is over estimated for learning
  real action uncertainty: 0.1
  over estimated action uncertainty: 0.2
  obstacles: [[4,0,0],[5,0,0],[4,1,0],[5,1,0],[2,3,0],[2,4,0],[2,5,0],[3,3,0],[3,4,0],[3,5,0],[4,3,0],[4,4,0],[4,5,0]]

TWTL constraint:
  # You can define region coordinates that will replace the respective name in the TWTL task.
  # You will have issues if one name is a subset of another (i.e. 'base' and 'base2')
  regions:
    pickup: [5,6,0]
    delivery1: [7,1,0]
    delivery2: [3,0,0]
    base: [0,7,0]
  custom task: '[H^1 pickup]^[0,20] * ([H^1 delivery1]^[0,20] | [H^1 delivery2]^[0,20]) * [H^1 base]^[0,20]'
  # custom task: 'None'
  desired satisfaction probability: 0.8
  time horizon: 30

# MDP type: flag-MDP
# MDP type: tau-MDP
MDP type: static rewards

# Used for flag-MDP and tau-MDP augmentations
aug-MDP rewards:
  STL expression: G[0,10]F[0,3](((x>1)&(x<2))&((y>3)&(y<4)))

static rewards:
  reward dict: 
    r0: 1
    r1: 1
    r2: 1
    r8: 1
    r9: 10
    r10: 1
    r16: 1
    r17: 1
    r18: 1
    r58: 1
    r59: 1

Q-learning config:
  number of episodes: 50000
  learning rate: 0.1
  discount: 0.999
  # Exploration probability decay is calculated from the following values as (start * decay^(num_eps - 1) = end)
  explore probability start: 0.4
  explore probability end: 0.07
